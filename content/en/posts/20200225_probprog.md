---
title: "확률적 프로그래밍 기초 원리"
author: "Hoontaek Lee"
date: 2020-02-25T20:54:40+09:00
lastmod: 2020-02-25T21:04:56+09:00
description:
draft: false
hideToc: false
enableToc: true
enableTocContent: false
tocPosition: inner
tocLevels: ["h2", "h3", "h4"]
tags:
- Book Review
- 2020
---

<img src="https://image.aladin.co.kr/product/6696/44/cover500/8960777641_1.jpg" style="zoom:50%;" />

## Intro

요즘 종종 **베이지안 네트워크(Bayesian network, BN)를** 활용하는 논문이 보인다. 예를 들어 산불관련 논문에서는 산불 발생과 관련된 인위적, 자연적 인자를 나열한 후 각 인자 및 인자간 조합에 따른 산불 발생 확률을 분석했다. 

**베이지안 네트워크**에 흥미를 조금씩 가져가던 차에 과학원 도서관에서 이 책을 발견했다. 총 4부 중 실습과 관련된 3, 4부를 제외하고 1, 2부만 읽어보기로 했다. 개략적으로 살펴보는 게 목적이니까.



## Bayesian network

### Description

간단히 정리하면

**베이지안 네트워크란** 특정 상황에 대한 지식들을 원인과 결과의 형태로 표기하는 그래프를 말한다. 베이지안 네트워크는 명제를 벽돌삼는 몇 가지 형태(순차, 발산, 수렴)의 노드로 이루어져 있다. 이 벽돌을 구성하는 개념(확률, 조건부 확률, 진리표, 주관적 확률 등)은 1장~8장에서 설명하고 있다. 네트워크를 이루는 **각 노드의 발생 확률을 정량적으로 구해서 이해하기 쉽게 표현하고 예측 및 추론에 사용**할 수 있는 도구다.

인자가 많아질 수록 네트워크가 복잡해질 수 있는데, 베이지안 네트워크에서는 각 노드 간 독립성(자신, 부모, 자손을 제외한 모든 노드) 성질을 이용해 **네트워크를 단순화**할 수 있다.



### Why BN?

이와 관련된 설명은 7, 8장에 잘 나와있다. 결론적으로 BN에서 사용하는 **베이지안 확률(주관적 확률)로 현실을, 사람의 사고 과정을 잘 표현할 수 있기 때문**이다.



#### 객관적 확률과 주관적 확률

**객관적 확률**에서는 참, 거짓 혹은 그 사건이 발생할 확률이 딱딱 계산된다. 반면, **주관적 확률**(vs 객관적 확률)에서는 사람마다 느끼는 **사전 확률**이 사건 발생 후 **사후 확률**로 수정되는 과정이 발생한다.



#### 베이지안의 현실성

실제로 **사람들은 베이지안과 비슷하게 추론**한다. 증거를 본 후 기존 믿음에 변화를 준다. 

게다가 현실에서는 참/거짓으로 딱딱 나눠지는 상황, 단순한 인과관계를 갖는 상황이 아닌 경우가 많다. 참인 것 같으면서 어느 조건이 만족하면 거짓이 되는 등... 매우 복잡하다. 확률과 명제를 함께 사용하면 더 많은, 복잡한 정보를 **현실에 가깝게 표현할 수 있다**.



## Bayes theorem & likelihood

- 베이지안 정리 및 가능도(우도, likelihood)를 이해하는 데 조금 도움이 됐다. **도움이 됐다**는 것은 이 책이 특별히 설명을 잘 해서라기보다는 여러 번 이 정리와 용어를 봐오던 중 지금 우연히 이해도가 올라갔을 수 있다. **조금** 도움이 됐다는 것은 이 책이 설명을 못했다기보다는 원래 저 용어와 정리가 직관적으로 받아들이기 힘들다는 뜻이다.
- 베이지안 정리는 **사후확률 = 가능도 x 사전확률 / 증거**로 쓸 수 있다. 수두가 발생하는 사건이 H, 붉은 반점이 보이는 사건이 D라고 하자. **구하고 싶은 것은 P(H|D)**, 즉 붉은 반점이 보일 때 수두일 확률이다. 베이지안 정리로 풀면 **P(H|D) = P(D|H) x P(H) / P(D)이다**. 수두는 한 천 명에 한 명 걸리나(사전확률)? 여기에 수두에 걸린 사람이 그 증거(붉은 반점)를 보일 확률(내 사전 믿음에서 해당 증거가 발견 될 가능성, likelihood)을 곱하고, 이것을 증거가 발생할 확률로 **스케일링**해준다.
- 스케일링 해준다는 것은 전체 H집합 중 D에 해당하는 것으로 상정 범위를 좁혀준다는 뜻이다. **사후확률오즈**(두 베이지안 모델의 사후확률 비율)를 구할 때는 고려하지 않아도 된다. 두 모델 모두 P(D)를 분모로 하여 계산 중에 약분되기 때문이다. 이런 점에서도 P(D)로 나누는 것은 단순히 스케일을 조정하는 기능(표준편차 구할 때 제곱근을 구하는 것처럼)으로 볼 수 있을 것 같다.



## Others

- **6장 초반에 나오는 가능도에 대한 설명은 틀린 것 같다**. 주어진 데이터를 기반으로 가설이 맞을 확률을 구하기 때문에 **가능도 = P(H|D)라고** 써 있는데, 앞 장의 표기방식이 변한 게 아니라면 **가능도는 P(D|H)가** 맞는 것 같다.
- **설명이 친절하다**. 개념, 상황 등 설명을 정의 한 줄, 요약 한 줄로 끝내는 게 아니라 정의, 여러 예시와 자세한 풀이, 뒤에서 한 번 더 요약하고 다른 장에서 반복하기까지... 나는 이렇게 말하듯 자세히 풀어 설명해주는 게 좋다.
- **요약이 잘 돼있다**. 첫 번째와 비슷한 장점이다. 장 끝마다 몇 포인트로 요약을 해뒀는데, 이것만 가볍게 읽어도 감을 잡을 수 있을 정도다.



| 제목                        | 저자   | 출판사 | 판   | 출간          | 완독          |
| --------------------------- | ------ | ------ | ---- | ------------- | ------------- |
| 확률적 프로그래밍 기초 원리 | 신승환 | 에이콘 | 1    | 2015. 09. 21. | 2020. 02. 24. |